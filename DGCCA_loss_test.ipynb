{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DGCCA-loss-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsbWc5PBspg2E+fjIXsxWl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arminarj/DGCCA-pytorch/blob/master/DGCCA_loss_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PcxwzgXQODv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tUMogAE7fLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GCCA_loss(H1, H2):\n",
        "    r = 1e-4\n",
        "    eps = 1e-8\n",
        "\n",
        "    # H1, H2, H3 = H1.t(), H2.t(), H3.t()\n",
        "\n",
        "    # print(f'H1 shape ( N X feature) : {H1.shape}')\n",
        "\n",
        "    assert torch.isnan(H1).sum().item() == 0 \n",
        "    assert torch.isnan(H2).sum().item() == 0\n",
        "    # assert torch.isnan(H3).sum().item() == 0\n",
        "\n",
        "    o1 = H1.size(0)  # N\n",
        "    o2 = H2.size(0)\n",
        "    m = H1.size(1)   # out_dim\n",
        "\n",
        "    top_k = 10\n",
        "\n",
        "    H1bar = H1 - H1.mean(dim=1).repeat(m, 1).view(-1, m)\n",
        "    H2bar = H2 - H2.mean(dim=1).repeat(m, 1).view(-1, m)\n",
        "    assert torch.isnan(H1bar).sum().item() == 0\n",
        "    assert torch.isnan(H2bar).sum().item() == 0\n",
        "\n",
        "    A1, S1, B1 = H1bar.svd(some=True, compute_uv=True)\n",
        "    A2, S2, B2 = H2bar.svd(some=True, compute_uv=True)\n",
        "\n",
        "    A1, A2 = A1[:, :top_k], A2[:, :top_k]\n",
        "\n",
        "    assert torch.isnan(A1).sum().item() == 0\n",
        "    assert torch.isnan(A2).sum().item() == 0\n",
        "\n",
        "    S_thin_1, S_thin_2 = S1[:top_k], S2[:top_k]\n",
        "\n",
        "\n",
        "    S2_inv_1 = 1. / (torch.mul( S_thin_1, S_thin_1 ) + eps)\n",
        "    S2_inv_2 = 1. / (torch.mul( S_thin_2, S_thin_2 ) + eps)\n",
        "\n",
        "    assert torch.isnan(S2_inv_1).sum().item() == 0\n",
        "    assert torch.isnan(S2_inv_2).sum().item() == 0\n",
        "\n",
        "    T2_1 = torch.mul( torch.mul( S_thin_1, S2_inv_1 ), S_thin_1 )\n",
        "    T2_2 = torch.mul( torch.mul( S_thin_2, S2_inv_2 ), S_thin_2 )\n",
        "\n",
        "    assert torch.isnan(T2_1).sum().item() == 0\n",
        "    assert torch.isnan(T2_2).sum().item() == 0\n",
        "\n",
        "    T2_1 = torch.where(T2_1>eps, T2_1, (torch.ones(T2_1.shape)*eps).to(H1.device).double())\n",
        "    T2_2 = torch.where(T2_2>eps, T2_2, (torch.ones(T2_2.shape)*eps).to(H2.device).double())\n",
        "\n",
        "\n",
        "    T_1 = torch.diag(torch.sqrt(T2_1))\n",
        "    T_2 = torch.diag(torch.sqrt(T2_2))\n",
        "\n",
        "    assert torch.isnan(T_1).sum().item() == 0\n",
        "    assert torch.isnan(T_2).sum().item() == 0\n",
        "\n",
        "    T_unnorm_1 = torch.diag( S_thin_1 + eps )\n",
        "    T_unnorm_2 = torch.diag( S_thin_2 + eps )\n",
        "\n",
        "    assert torch.isnan(T_unnorm_1).sum().item() == 0\n",
        "    assert torch.isnan(T_unnorm_2).sum().item() == 0\n",
        "\n",
        "    AT_1 = torch.mm(A1, T_1)\n",
        "    AT_2 = torch.mm(A2, T_2)\n",
        "\n",
        "    M_tilde = torch.cat([AT_1, AT_2], dim=1)\n",
        "\n",
        "    # print(f'M_tilde shape : {M_tilde.shape}')\n",
        "\n",
        "    assert torch.isnan(M_tilde).sum().item() == 0\n",
        "\n",
        "    Q, R = M_tilde.qr()\n",
        "\n",
        "    assert torch.isnan(R).sum().item() == 0\n",
        "    assert torch.isnan(Q).sum().item() == 0\n",
        "\n",
        "    U, lbda, _ = R.svd(some=False, compute_uv=True)\n",
        "\n",
        "    assert torch.isnan(U).sum().item() == 0\n",
        "    assert torch.isnan(lbda).sum().item() == 0\n",
        "\n",
        "    G = Q.mm(U[:,:top_k])\n",
        "    assert torch.isnan(G).sum().item() == 0\n",
        "\n",
        "\n",
        "    U = [] # Mapping from views to latent space\n",
        "\n",
        "    # Get mapping to shared space\n",
        "    views = [H1, H2]\n",
        "    F = [o1, o2] # features per view\n",
        "    for idx, (f, view) in enumerate(zip(F, views)):\n",
        "        _, R = torch.qr(view)\n",
        "        Cjj_inv = torch.inverse( (R.T.mm(R) + eps * torch.eye( view.shape[1], device=view.device)) )\n",
        "        assert torch.isnan(Cjj_inv).sum().item() == 0\n",
        "        pinv = Cjj_inv.mm( view.T)\n",
        "            \n",
        "        U.append(pinv.mm( G ))\n",
        "\n",
        "    U1, U2  = U[0], U[1]\n",
        "    _, S, _ = M_tilde.svd(some=True)\n",
        "\n",
        "    assert torch.isnan(S).sum().item() == 0\n",
        "    use_all_singular_values = False\n",
        "    if not use_all_singular_values:\n",
        "        S = S.topk(top_k)[0]\n",
        "    corr = torch.sum(S )\n",
        "    assert torch.isnan(corr).item() == 0\n",
        "    # loss = 14.1421-corr\n",
        "    loss = - corr\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCS80-vPAu7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GCCA_loss(H_list):\n",
        "\n",
        "    r = 1e-4\n",
        "    eps = 1e-8\n",
        "\n",
        "    # H1, H2, H3 = H1.t(), H2.t(), H3.t()\n",
        "\n",
        "    # print(f'H1 shape ( N X feature) : {H1.shape}')\n",
        "\n",
        "    # assert torch.isnan(H1).sum().item() == 0 \n",
        "    # assert torch.isnan(H2).sum().item() == 0\n",
        "    # assert torch.isnan(H3).sum().item() == 0\n",
        "\n",
        "    # o1 = H1.size(0)  # N\n",
        "    # o2 = H2.size(0)\n",
        "\n",
        "    top_k = 10\n",
        "\n",
        "    AT_list =  []\n",
        "\n",
        "    for H in H_list:\n",
        "        assert torch.isnan(H).sum().item() == 0 \n",
        "\n",
        "        o_shape = H.size(0)  # N\n",
        "        m = H.size(1)   # out_dim\n",
        "\n",
        "        # H1bar = H1 - H1.mean(dim=1).repeat(m, 1).view(-1, m)\n",
        "        Hbar = H - H.mean(dim=1).repeat(m, 1).view(-1, m)\n",
        "        assert torch.isnan(Hbar).sum().item() == 0\n",
        "\n",
        "        A, S, B = Hbar.svd(some=True, compute_uv=True)\n",
        "\n",
        "        A = A[:, :top_k]\n",
        "\n",
        "        assert torch.isnan(A).sum().item() == 0\n",
        "\n",
        "        S_thin = S[:top_k]\n",
        "\n",
        "        S2_inv = 1. / (torch.mul( S_thin, S_thin ) + eps)\n",
        "\n",
        "        assert torch.isnan(S2_inv).sum().item() == 0\n",
        "\n",
        "        T2 = torch.mul( torch.mul( S_thin, S2_inv ), S_thin )\n",
        "\n",
        "        assert torch.isnan(T2).sum().item() == 0\n",
        "\n",
        "        T2 = torch.where(T2>eps, T2, (torch.ones(T2.shape)*eps).to(H.device).double())\n",
        "\n",
        "\n",
        "        T = torch.diag(torch.sqrt(T2))\n",
        "\n",
        "        assert torch.isnan(T).sum().item() == 0\n",
        "\n",
        "        T_unnorm = torch.diag( S_thin + eps )\n",
        "\n",
        "        assert torch.isnan(T_unnorm).sum().item() == 0\n",
        "\n",
        "        AT = torch.mm(A, T)\n",
        "        AT_list.append(AT)\n",
        "\n",
        "    M_tilde = torch.cat(AT_list, dim=1)\n",
        "\n",
        "    # print(f'M_tilde shape : {M_tilde.shape}')\n",
        "\n",
        "    assert torch.isnan(M_tilde).sum().item() == 0\n",
        "\n",
        "    Q, R = M_tilde.qr()\n",
        "\n",
        "    assert torch.isnan(R).sum().item() == 0\n",
        "    assert torch.isnan(Q).sum().item() == 0\n",
        "\n",
        "    U, lbda, _ = R.svd(some=False, compute_uv=True)\n",
        "\n",
        "    assert torch.isnan(U).sum().item() == 0\n",
        "    assert torch.isnan(lbda).sum().item() == 0\n",
        "\n",
        "    G = Q.mm(U[:,:top_k])\n",
        "    assert torch.isnan(G).sum().item() == 0\n",
        "\n",
        "\n",
        "    U = [] # Mapping from views to latent space\n",
        "\n",
        "    # Get mapping to shared space\n",
        "    views = H_list\n",
        "    F = [H.shape[0] for H in H_list] # features per view\n",
        "    for idx, (f, view) in enumerate(zip(F, views)):\n",
        "        _, R = torch.qr(view)\n",
        "        Cjj_inv = torch.inverse( (R.T.mm(R) + eps * torch.eye( view.shape[1], device=view.device)) )\n",
        "        assert torch.isnan(Cjj_inv).sum().item() == 0\n",
        "        pinv = Cjj_inv.mm( view.T)\n",
        "            \n",
        "        U.append(pinv.mm( G ))\n",
        "\n",
        "    U1, U2  = U[0], U[1]\n",
        "    _, S, _ = M_tilde.svd(some=True)\n",
        "\n",
        "    assert torch.isnan(S).sum().item() == 0\n",
        "    use_all_singular_values = False\n",
        "    if not use_all_singular_values:\n",
        "        S = S.topk(top_k)[0]\n",
        "    corr = torch.sum(S )\n",
        "    assert torch.isnan(corr).item() == 0\n",
        "    # loss = 14.1421-corr\n",
        "    loss = - corr\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_XEQTf7FIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MlpNet(nn.Module):\n",
        "    def __init__(self, layer_sizes, input_size):\n",
        "        super(MlpNet, self).__init__()\n",
        "        layers = []\n",
        "        layer_sizes = [input_size] + layer_sizes\n",
        "        for l_id in range(len(layer_sizes) - 1):\n",
        "            if l_id == len(layer_sizes) - 2:\n",
        "                layers.append(nn.Sequential(\n",
        "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id + 1]),\n",
        "                    nn.Sigmoid(), \n",
        "                    nn.BatchNorm1d(num_features=layer_sizes[l_id + 1], affine=False),\n",
        "                    \n",
        "                ))\n",
        "            else:\n",
        "                layers.append(nn.Sequential(\n",
        "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id + 1]),\n",
        "                    nn.ReLU(),\n",
        "                    # nn.BatchNorm1d(num_features=layer_sizes[l_id + 1], affine=True),\n",
        "                ))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "class DeepGCCA(nn.Module):\n",
        "    def __init__(self, layer_sizes1, layer_sizes2, input_size1, input_size2, outdim_size, use_all_singular_values, device=torch.device('cpu')):\n",
        "        super(DeepGCCA, self).__init__()\n",
        "        self.model1 = MlpNet(layer_sizes1, input_size1).double()\n",
        "        self.model2 = MlpNet(layer_sizes2, input_size2).double()\n",
        "        self.model3 = MlpNet(layer_sizes2, input_size2).double()\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        \"\"\"\n",
        "\n",
        "        x1, x2 are the vectors needs to be make correlated\n",
        "        dim=[batch_size, feats]\n",
        "\n",
        "        \"\"\"\n",
        "        # feature * batch_size\n",
        "        output1 = self.model1(x1)\n",
        "        output2 = self.model2(x2)\n",
        "        output3 = self.model3(x3)\n",
        "\n",
        "        return output1, output2, output3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kK_5UIV6ofB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-2\n",
        "device = 'cpu'\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# size of the input for view 1 and view 2\n",
        "input_shape1 = 30\n",
        "input_shape2 = 30\n",
        "input_shape3 = 30\n",
        "\n",
        "X1 = torch.randn((1000, input_shape1), requires_grad=True).double().to(device)\n",
        "X2 = torch.randn((1000, input_shape2), requires_grad=True).double().to(device)\n",
        "X3 = torch.randn((1000, input_shape2), requires_grad=True).double().to(device)\n",
        "\n",
        "\n",
        "outdim_size = 20\n",
        "\n",
        "# number of layers with nodes in each one\n",
        "layer_sizes1 = [128, 128, outdim_size]\n",
        "layer_sizes2 = [128, 128, outdim_size]\n",
        "layer_sizes3 = [128, 128, outdim_size]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_ZRoVWWSNIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy as copy\n",
        "\n",
        "model = DeepGCCA(layer_sizes1, layer_sizes2, input_shape1, input_shape2, outdim_size, False, device).double().to(device)\n",
        "lr  = 1e-2\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "criterion = GCCA_loss\n",
        "\n",
        "train_loss = []\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(400):\n",
        "    optimizer.zero_grad()\n",
        "    out1, out2, out3 = model(X1, X2, X3)\n",
        "    loss = criterion([out1, out2, out3])\n",
        "    # print(loss)\n",
        "    train_loss.append(copy(loss.data))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5zmmffM9nJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "ee92626e-7e6f-447b-f95b-fc9b8d5c5325"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "loss_plt = pd.DataFrame(train_loss)\n",
        "loss_plt.plot()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f18fa8e55f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dELtbHByC4I7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3ac37621-1c8d-4b1f-ab17-4b4574e3c448"
      },
      "source": [
        "criterion([X1, X2, X3])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-10.7722, dtype=torch.float64, grad_fn=<NegBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47uFfJdEDAWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d2e56e6e-f111-4a4a-e340-30b4407aa74f"
      },
      "source": [
        "print(criterion([X1, X1, X1]))\n",
        "print(criterion([X2, X2, X2]))\n",
        "print(criterion([X3, X3, X3]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-17.3205, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "tensor(-17.3205, dtype=torch.float64, grad_fn=<NegBackward>)\n",
            "tensor(-17.3205, dtype=torch.float64, grad_fn=<NegBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG9mDDt0DHWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}